{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f061b442-f917-42b8-a635-bd85bb0b502f",
   "metadata": {},
   "source": [
    "# MDIBL Transcriptome Assembly Learning Module\n",
    "# Notebook 3: Performing a \"Standard\" basic transcriptome assembly\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will run `denovotranscript` Nextflow pipeline utilizing AWS Batch. It is a bioinformatics pipeline for de novo transcriptome assembly of paired-end short reads from bulk RNA-seq. It takes a samplesheet and FASTQ files as input, perfoms quality control (QC), trimming, assembly, redundancy reduction, pseudoalignment, and quantification. It outputs a transcriptome assembly FASTA file, a transcript abundance TSV file, and a MultiQC report with assembly quality and read QC metrics.\n",
    "\n",
    "> <img src=\"../images/denovo-workflow.png\" width=\"1500\">\n",
    "\n",
    "1. Read QC of raw reads (FastQC)\n",
    "\n",
    "2. Adapter and quality trimming (fastp)\n",
    "\n",
    "3. Read QC of trimmed reads (FastQC)\n",
    "\n",
    "4. Remove rRNA or mitochondrial DNA (optional) (SortMeRNA)\n",
    "\n",
    "5. Transcriptome assembly using any combination of the following:\n",
    "    - Trinity with normalised reads (default=True)\n",
    "    - Trinity with non-normalised reads\n",
    "    - rnaSPAdes medium filtered transcripts outputted (default=True)\n",
    "    - rnaSPAdes soft filtered transcripts outputted\n",
    "    - rnaSPAdes hard filtered transcripts outputted\n",
    "\n",
    "6. Redundancy reduction with Evidential Gene tr2aacds. A transcript to gene mapping is produced from Evidential Gene’s outputs using gawk.\n",
    "\n",
    "7. Assembly completeness QC (BUSCO)\n",
    "\n",
    "8. Other assembly quality metrics (rnaQUAST)\n",
    "\n",
    "9. Transcriptome quality assessment with TransRate, including the use of reads for assembly evaluation. This step is not performed if profile is set to conda or mamba.\n",
    "\n",
    "10. Pseudo-alignment and quantification (Salmon)\n",
    "\n",
    "11. HTML report for raw reads, trimmed reads, BUSCO, and Salmon (MultiQC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50574500",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c79e41",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**1. Software/Environment:**\n",
    "\n",
    "*   Jupyter Notebook in AWS SageMaker AI (Python kernel)\n",
    "*   AWS CLI (configured)\n",
    "*   Nextflow (installed via `mamba`, switch kernel to `conda_nextflow`)\n",
    "*   `jupytercards` (install via `pip`)\n",
    "\n",
    "**2. Enabled APIs:**\n",
    "\n",
    "*   AWS Batch\n",
    "*   Amazon S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0f4f2-5559-4675-9e97-24b0548b31af",
   "metadata": {},
   "source": [
    "## Get Started "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b740150",
   "metadata": {},
   "source": [
    "### **Step 1:** Data\n",
    "\n",
    "The data used for this notebook is stored in a public Amazon S3 bucket: `s3://nigms-sandbox/nosi-inbremaine-storage/resources`. While you *could* download the data to your local machine for more detailed inspection, as you did in submodule-2, it is not *required*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c3682-deae-4eb3-a10b-c9c2be24852d",
   "metadata": {},
   "source": [
    "First, check the listings within the `resources directory`. Make sure you see the items listed below:\n",
    "```\n",
    "DBs  bin  conf  seq2  trans\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc95c9c-d19a-479c-8aa3-101191a00c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls s3://nigms-sandbox/nosi-inbremaine-storage/resources/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276e979-b4eb-40f0-b336-c41d3391549a",
   "metadata": {},
   "source": [
    "Now, check the listing of the sequence directory: `seq2`. You should see seven pairs of gzipped fastq files (signified by the paired `.fastq.gz` naming). Six of these are for individual samples, and the seventh set, labeled **joined** is a concatenation of all files. Because of the way that denovotrascript works (as well as some of the programs that it uses), it's best to use a joined set of all sequences to make a unified transcriptome assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba183a-0e68-4e23-9f98-1d18dec69aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls s3://nigms-sandbox/nosi-inbremaine-storage/resources/seq2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dd0c5",
   "metadata": {},
   "source": [
    "***If you have not set up AWS Batch please proceed to Step 2, otherwise proceed to Step 3.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87b0d2",
   "metadata": {},
   "source": [
    "### **Step 2:** Setting up AWS Batch \n",
    "\n",
    "AWS Batch manages the provisioning of compute environments (EC2, Fargate), container orchestration, job queues, IAM roles, and permissions. We can deploy a full environment either:\n",
    "- Automatically using a preconfigured AWS CloudFormation stack (**recommended**)\n",
    "- Manually by setting up roles, queues, and buckets\n",
    "The Launch Stack button below will take you to the cloud formation create stack webpage with the template with required resources already linked. \n",
    "\n",
    "If you prefer to skip manual deployment and deploy automatically in the cloud, click the **Launch Stack** button below. For a walkthrough of the screens during automatic deployment please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/HowToLaunchAWSBatch.md). The deployment should take ~5 min and then the resources will be ready for use. \n",
    "\n",
    "[![Launch Stack](../images/LaunchStack.jpg)](https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=aws-batch-nigms&templateURL=https://nigms-sandbox.s3.us-east-1.amazonaws.com/cf-templates/AWSBatch_template.yaml )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55c633",
   "metadata": {},
   "source": [
    "### **Step 3:** Install dependencies, update paths and create a new S3 Bucket to store input and output files\n",
    "\n",
    "After setting up an AWS CloudFormation stack, we need to let the nextflow workflow to know where are those resrouces by providing the configuration:\n",
    "<div style=\"border: 1px solid #e57373; padding: 0px; border-radius: 4px;\">\n",
    "  <div style=\"background-color: #ffcdd2; padding: 5px; \">\n",
    "    <i class=\"fas fa-exclamation-triangle\" style=\"color: #b71c1c;margin-right: 5px;\"></i><a style=\"color: #b71c1c\"><b>Important</b> - Customize Required</a>\n",
    "  </div>\n",
    "  <p style=\"margin-left: 5px;\">\n",
    "After successfull creation of your stack you must attatch a new role to SageMaker to be able to submit batch jobs. Please following the the following steps to change your SageMaker role:<br>\n",
    "<ol> <li>Navigate to your SageMaker AI notebook dashboard (where you initially created and launched your VM)</li> <li>Locate your instance and click the <b>Stop</b> button</li> <li>Once the instance is stopped: <ul> <li>Click <b>Edit</b></li> <li>Scroll to the \"Permissions and encryption\" section</li> <li>Click the IAM role dropdown</li> <li>Select the new role created during stack formation (named something like <b>aws-batch-nigms-SageMakerExecutionRole</b>)</li> </ul> </li> \n",
    "<li>Click <b>Update notebook instance</b> to save your changes</li> \n",
    "<li>After the update completes: <ul> <li>Click <b>Start</b> to relaunch your instance</li> <li>Reconnect to your instance</li> <li>Resume your work from this point</li> </ul> </li> </ol>\n",
    "\n",
    "<b>Warning:</b> Make sure to replace the <b>stack name</b> to the stack that you just created. <code>STACK_NAME = \"your-stack-name-here\"</code>\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a stack name variable\n",
    "STACK_NAME = \"aws-batch-nigms-test1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Get account ID and region \n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variable names \n",
    "# These variables should come from the Intro AWS Batch tutorial (or leave as-is if using the launch stack button)\n",
    "BUCKET_NAME = f\"{STACK_NAME}-batch-bucket-{account_id}\"\n",
    "AWS_QUEUE = f\"{STACK_NAME}-JobQueue\"\n",
    "INPUT_FOLDER = 'nigms-sandbox/nosi-inbremaine-storage/'\n",
    "AWS_REGION = region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce8e92",
   "metadata": {},
   "source": [
    "#### Install dependencies\n",
    "Installs Nextflow and Java, which are required to execute the pipeline. In environments like SageMaker, Java is usually pre-installed. But if you're running outside SageMaker (e.g., EC2 or local), you’ll need to manually install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7625b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Nextflow\n",
    "! mamba install -y -c conda-forge -c bioconda nextflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b91ef0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Install Java and Nextflow if needed in other systems</summary>\n",
    "If using other system other than AWS SageMaker Notebook, you might need to install java and nextflow using the code below:\n",
    "<br> <i># Install java</i><pre>\n",
    "    sudo apt update\n",
    "    sudo apt-get install default-jdk -y\n",
    "    java -version\n",
    "    </pre>\n",
    "    <i># Install Nextflow</i><pre>\n",
    "    curl https://get.nextflow.io | bash\n",
    "    chmod +x nextflow\n",
    "    ./nextflow self-update\n",
    "    ./nextflow plugin update\n",
    "    </pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f28ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace batch bucket name in nextflow configuration file\n",
    "! sed -i \"s/aws-batch-nigms-batch-bucket-/$BUCKET_NAME/g\" ../denovotranscipt/nextflow.config\n",
    "# replace job queue name in configuration file \n",
    "! sed -i \"s/aws-batch-nigms-JobQueue/$AWS_QUEUE/g\" ../denovotranscipt/nextflow.config\n",
    "# replace the region placeholder with the region you are in \n",
    "! sed -i \"s/aws-region/$AWS_REGION/g\" ../denovotranscipt/nextflow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1a3b8",
   "metadata": {},
   "source": [
    "### **Step 4:** Enable AWS Batch for the nextflow script `denovotranscript`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb30de",
   "metadata": {},
   "source": [
    "Run the pipeline in a cloud-native, serverless manner using AWS Batch. AWS Batch offloads the burden of provisioning and managing compute resources. When you execute this command:\n",
    "- Nextflow uploads tasks to AWS Batch. \n",
    "- AWS Batch pulls the necessary containers.\n",
    "- Each process/task in the pipeline runs as an isolated job in the cloud.\n",
    "\n",
    "The beauty and power of using a defined workflow in a management system (such as Nextflow) are that we not only get a defined set of steps that are carried out in the proper order, but we also get a well-structured and concise directory structure that holds all pertinent output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5985e3-93df-4779-afe1-4464e13bf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run main.nf --input test_samplesheet.csv -profile aws --run_mode full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc1de0",
   "metadata": {},
   "source": [
    "With the execution complete, let's look at what we have generated, first in the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5a924-fbfc-49a9-ad8e-db707bca1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls s3://$BUCKET_NAME/nextflow_output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d927ba-6916-47b5-8edf-1cc58d585f78",
   "metadata": {},
   "source": [
    "## Investigation and Exploration: Assembly and Annotation Results\n",
    "The use of an established and complex multi-step workflow (such as the TransPi workflow that you just ran) has the benefit of saving you a lot of manual effort in setting up and carrying out the individual steps yourself. It also is highly reproducible, given the same input data and parameters.\n",
    "\n",
    "It does, however, generate a lot of output, and it is beyond the scope of this training exercise to go through all of it in detail. We recommend that you download the complete results directory onto another machine or storage so that you can view it at your convenience, and on a less expensive machine than you are using to run this tutorial. *If you would like the proceed with the data in its current location, this also works, just bear in mind that it will cost roughly $0.72 per hour.*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  To Download...\n",
    "</div>\n",
    "\n",
    ">Here are two possible options to access the results files outside of this expensive JupyterLab instance.  \n",
    ">- If you instead have an external machine that accepts ssh connections, then you can use the secure copy scp command: `!scp -r ./basicRun/output/YOUR_USERID@YOUR.MACHINE`\n",
    ">- If you have a Google Cloud Storage bucket available, you can use the gsutil command: `!gsutil -m cp -r ./basicRun/output gs://YOUR-BUCKET-NAME-HERE/transpi-basicrun-output` to place all of your results into that bucket. \n",
    ">    - From there you have two options: \n",
    ">         1. (Recommended) You could create a new (cheaper) Vertex AI instance (or use an old one) and copy the files down into that new directory using the following gsutil command:`!gsutil -m cp -r gs://YOUR-BUCKET-NAME-HERE/transpi-basicrun-output ./`\n",
    ">         2. You could navigate to the bucket through the Google Cloud console and open the files through the links labeled `Authenticated URL`\n",
    ">\n",
    ">**In all of the commands above, you will need to edit the All-Caps part to match your own bucket or machine.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-lightbulb-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Tip: </b>\n",
    "</div>\n",
    "\n",
    "> - **After you have the output directory in its desired location, consider the information in the cell below as you explore the output.**\n",
    "> - **If you are viewing the output in a different location, consider copying or taking a screenshot of the cell below.**\n",
    "> - **Make sure that if you are viewing your output in a different location, you save your notebooks here, and then stop the VM instance, or it will keep costing money.**\n",
    "> - **Upon completion of your exploration, return to this submodule to complete the checkpoint quiz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c027e-edaf-4f96-9502-c4e4cfd67471",
   "metadata": {},
   "source": [
    "## Output Overview\n",
    "*These sub-directories will be mentioned in the order of their execution within TransPi.*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  HTML files\n",
    "</div>\n",
    "\n",
    ">**If you are viewing your output within a JupyterLab VM instance, for the `.html` files to work correctly, you will need to select `Trust HTML` at the top of the screen.** This is due to the dynamic elements within the files.\n",
    "\n",
    "### FastQC\n",
    "> FastQC takes the raw read files and runs a swift analysis of the read data. The two key output files are `joined_R1_fastqc.html` and `joined_R2_fastqc.html` which provide a visual illustration of the read quality metrics. It is important to note that FastQC does not manipulate the data for further steps, it just outputs an analysis of the data quality.\n",
    "\n",
    "### Filter\n",
    "> FastP is a bioinformatics tool that preprocesses the raw read data. It trims poor-quality reads, removes adapter sequences, and corrects errors noticed within the reads. The `joined.fastp.html` provides an overview of the processing done on both read files.\n",
    "\n",
    "### Assemblies\n",
    "> TransPi uses five different assembly tools. All of the assembly `.fa` files are placed within the assemblies directory. For all of the assemblies except for Trinity, there are four `.fa` files: one for each of the *k*-mer length plus a compilation of all three. Trinity does not have the option to customize the *k*-mer size. Instead, it runs at a default `k=25`, therefore only having one assembly.\n",
    "\n",
    "### EviGene\n",
    "> At this point, we have a major overassembly of the transcriptome. We use a small piece of the EvidentialGene (EviGene) program known as tr2aacds which takes all of the assemblies and crunches them into a single, unified transcriptome. Within the evigene directory, there are two files: `joined.combined.fa` is all of the assemblies placed into the same file and`joined.combined.okay.fa` is the combined transcriptome after EviGene has reduced it down. In each header line, there is key information about the sequence.\n",
    ">> For example: `>SOAP.k17.C9429 58.0 evgclass=main,okay,match:SPADES.k43.NODE_313_length_1670_cov_12.047941_g161_i0,pct:100/100/.; aalen=392,75%,complete;`\n",
    ">>\n",
    ">> - This header indicates that this sequence was found in both the SOAP and SPADES assemblies.\n",
    ">> - The `eviclass=main` means that this sequence is the primary transcript, and there are alternates identified.\n",
    ">> - The `aalen=392` is the amino acid length of the sequence.\n",
    ">> - The `complete` means that it is a complete reading frame.\n",
    ">> - For more information on interpreting the headers from EviGene, reference the following [link](http://arthropods.eugenes.org/EvidentialGene/evigene/) in section 3.\n",
    "\n",
    "### BUSCO\n",
    "> BUSCO uses a database of known universal single-copy orthologs under a specific lineage (vertebrata in this case) and checks our assembled transcriptome for those sequences which it expects to find. BUSCO was run on both the TransPi assembly along with the assembly just done by Trinity. To visualize BUSCO's results, refer to the `short_summary.specific.vertebrata_odb10.joined.TransPi.bus4.txt` and `short_summary.specific.vertebrata_odb10.joined.Trinity.bus4.txt` files.\n",
    "\n",
    "### Mapping \n",
    "> One way to verify the quality of the assembly is to map the original input reads to the assembly (using an alignment program called bowtie2). There are two output files, one for the TransPi assembly and one for the Trinity exclusive assembly. These files are named `log_joined.combined.okay.fa.txt` and `log_joined.Trinity.fa.txt`.\n",
    "\n",
    "### rnaQUAST\n",
    "> rnaQUAST is another assembly assessment program. It provides statistics about the transcripts that have been produced. For a brief overview of the transcript statistics, refer to `joined_rnaQUAST.csv`.\n",
    "\n",
    "### TransDecoder \n",
    "> TransDecoder is a program that \"decodes\" the transcripts. First, it identifies open reading frames (ORFs). From there, it then will make predictions on what is likely to be coding regions. For statistics regarding TransDecoder, refer to the `joined_transdecoder.stats` file.\n",
    "\n",
    "### Trinotate\n",
    "> Trinotate uses the information regarding likely coding regions produced by TransDecoder to make predictions about potential protein function. It does this by cross-referencing the assembled transcripts to various databases such as pfam and hmmer. These annotations can be viewed in the `joined.trinotate_annotation_report.xls` file.\n",
    "\n",
    "### Report\n",
    "> Within `report` is one file: `TransPi_Report_joined.html`. This is an HTML file that combines the results throughout TransPi into a series of visual tables and figures.\n",
    ">> The sub-directories `stats` and `figures` are intermediary sub-directories that hold information to generate the report.\n",
    "\n",
    "### pipeline_info\n",
    "> One of the benefits of using Nexflow and a well-defined pipeline/workflow is that when the run is completed, we get a high-level summary of the execution timeline and resources. Two key files within this sub-directory are `transpi_timeline.html` and `transpi_report.html`. In the `transpi_timeline.html` file, you can see a graphic representation of the total execution time of the entire workflow, along with where in the process each of the programs was active. From this diagram, you can also infer the ***dependency*** ordering that is encoded into the TransPi workflow. For example, none of the assembly runs started until the process labeled **`normalize reads`** was complete because each of these is run on the normalized data, rather than the raw input. Similarly, **`evigene`**, the program that integrates and refines the output of all of the assembly runs doesn't start until all of the assembly processes are complete. Within the `transpi_report.html` file, you can get a view of the resources used and activities carried out by each process, including CPUs, RAM, input/output, container used, and more.\n",
    "\n",
    "### RUN-INFO.txt\n",
    "> `RUN-INFO.txt` provides the specific details of the run such as where the directories are and the versions of the various programs used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372f902-6138-4217-86a8-4b0002f5f387",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "    <b>Checkpoint 1:</b>\n",
    "</div>\n",
    "\n",
    "*The green cards below are interactive. Spend some time to consider the question and click on the card to check your answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234a1c1-d73a-4120-894e-c2a76106dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an install that you need to run once to allow the quizes to be functional.\n",
    "! pip install jupytercards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e23c5b-a6e7-4ad7-b69c-7dc42fa9bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytercards import display_flashcards\n",
    "display_flashcards('../quiz-material/02-cp1-1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb7f1e-0dad-4743-a5fe-25b9501c478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flashcards('../quiz-material/02-cp1-2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f6112",
   "metadata": {},
   "source": [
    "## Conclusion: Why Use AWS Batch?\n",
    "<table border=\"1\" cellpadding=\"8\" cellspacing=\"0\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Benefit</th>\n",
    "      <th>Explanation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><strong>Scalability</strong></td>\n",
    "      <td>Process large MeRIP-seq datasets with multiple jobs in parallel</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Reproducibility</strong></td>\n",
    "      <td>Ensures the exact same Docker containers and config are used every time</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Ease of Management</strong></td>\n",
    "      <td>No need to manually manage EC2 instances or storage mounts</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Integration with S3</strong></td>\n",
    "      <td>Input/output seamlessly handled via S3 buckets</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Running on AWS Batch is ideal when your dataset grows beyond what your local notebook or server can handleor when you want reproducible, cloud-native workflows that are easier to scale, share, and manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68484f3",
   "metadata": {},
   "source": [
    "## Clean Up the AWS Environment\n",
    "\n",
    "Once you've successfully run your analysis and downloaded the results, it's a good idea to clean up unused resources to avoid unnecessary charges.\n",
    "\n",
    "#### Recommended Cleanup Steps:\n",
    "\n",
    "- **Delete Output Files from S3 (Optional)**  \n",
    "    If you've downloaded your results locally and no longer need them stored in the cloud.\n",
    "- **Delete the S3 Bucket (Optional)**    \n",
    "  To remove the entire bucket (only do this if you're sure!)\n",
    "- **Shut Down AWS Batch Resources (Optional but Recommended):**    \n",
    "  If you used a CloudFormation stack to set up AWS Batch, you can delete all associated resources in one step (⚠️ Note: Deleting the stack will also remove IAM roles and compute environments created by the template.):\n",
    "  + Go to the <a href=\"https://console.aws.amazon.com/cloudformation/\">AWS CloudFormation Console</a>\n",
    "  + Select your stack (e.g., <code>aws-batch-nigms-test1</code>)\n",
    "  + Click Delete\n",
    "  + Wait for all resources (compute environments, roles, queues) to be removed\n",
    "  \n",
    "<div style=\"border: 1px solid #659078; padding: 0px; border-radius: 4px;\">\n",
    "  <div style=\"background-color: #d4edda; padding: 5px; font-weight: bold;\">\n",
    "    <i class=\"fas fa-lightbulb\" style=\"color: #0e4628;margin-right: 5px;\"></i><a style=\"color: #0e4628\">Tips</a>\n",
    "  </div>\n",
    "  <p style=\"margin-left: 5px;\">\n",
    "It’s always good practice to periodically review your <b>EC2 instances</b>, <b>ECR containers</b>, <b>S3 storage</b>, and <b>CloudWatch logs</b> to ensure no stray resources are incurring charges.\n",
    "  </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
