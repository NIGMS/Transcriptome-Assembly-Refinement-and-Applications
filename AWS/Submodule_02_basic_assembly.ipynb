{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f061b442-f917-42b8-a635-bd85bb0b502f",
   "metadata": {},
   "source": [
    "# MDIBL Transcriptome Assembly Learning Module\n",
    "# Notebook 2: Performing a \"Standard\" basic transcriptome assembly\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will set up and run a basic transcriptome assembly, using the analysis pipeline as defined by the TransPi Nextflow workflow. The steps to be carried out are the following, and each is described in more detail in the Background material notebook.\n",
    "\n",
    "- Sequence Quality Control (QC): removing adapters and low-quality sequences.\n",
    "- Sequence normalization: reducing the reads that appear to be \"overrepresented\" (based on their *k*-mer content).\n",
    "- Generation of multiple 1st-pass assemblies using the following tools: Trinity, TransAbyss, SOAP, rnaSpades, and Velvet/Oases.\n",
    "- Integration and reduction of the individual transcriptomes using EvidentialGene.\n",
    "- Assessment of the final transcriptome with rnaQuast and BUSCO.\n",
    "- Annotation of the final transcriptome using alignment to known proteins (using DIAMOND/BLAST) and assignment to probable protein domains (using HMMER/Pfam).\n",
    "- Generation of output reports.\n",
    "\n",
    "> <img src=\"../images/TransPiWorkflow.png\" width=\"1500\">\n",
    ">\n",
    "> **Figure 1:** TransPi workflow for a basic transcriptome assembly run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062784ec",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. **Understanding the TransPi Workflow:** Learners will gain a conceptual understanding of the TransPi workflow, including its individual steps and their order.  This involves understanding the purpose of each stage (QC, normalization, assembly, integration, assessment, annotation, and reporting).\n",
    "\n",
    "2. **Executing a Transcriptome Assembly:** Learners will learn how to run a transcriptome assembly using Nextflow and the TransPi pipeline, including setting necessary parameters (e.g., k-mer size, read length). They will learn how to interpret the command-line interface for executing Nextflow workflows.\n",
    "\n",
    "3. **Interpreting Nextflow Output:** Learners will learn to navigate and understand the directory structure generated by the TransPi workflow.  This includes interpreting the output from various tools such as FastQC, FastP, Trinity, TransAbyss, SOAP, rnaSpades, Velvet/Oases, EvidentialGene, rnaQuast, BUSCO, DIAMOND/BLAST, HMMER/Pfam, and TransDecoder.  This involves understanding the different types of output files generated and how to extract relevant information from them (e.g., assembly statistics, annotation results).\n",
    "\n",
    "4. **Assessing Transcriptome Quality:** Learners will understand how to assess the quality of a transcriptome assembly using metrics generated by rnaQuast and BUSCO.\n",
    "\n",
    "5. **Interpreting Annotation Results:** Learners will learn to interpret the results of transcriptome annotation using tools like DIAMOND/BLAST and HMMER/Pfam, understanding what information they provide regarding protein function and domains.\n",
    "\n",
    "6. **Utilizing Workflow Management Systems:** Learners will gain practical experience using Nextflow, a workflow management system, to execute a complex bioinformatics pipeline.  This includes understanding the benefits of using a defined workflow for reproducibility and efficiency.\n",
    "\n",
    "7. **Working with Jupyter Notebooks:** The notebook itself provides a practical example of how to integrate command-line tools within a Jupyter Notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9345c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* **Nextflow:** A workflow management system used to execute the TransPi pipeline. \n",
    "* **Docker:** Used for containerization of the various bioinformatics tools within the workflow.  This avoids the need for local installation of numerous packages.\n",
    "* **TransPi:** The specific Nextflow pipeline for transcriptome assembly. The notebook assumes it's present in the `/home/jupyter` directory.\n",
    "* **Bioinformatics Tools (within TransPi):** The workflow utilizes several bioinformatics tools. These are packaged within Docker containers, but the notebook expects that TransPi is configured correctly to access and use them:\n",
    "    * FastQC: Sequence quality control.\n",
    "    * FastP: Read preprocessing (trimming, adapter removal).\n",
    "    * Trinity, TransAbyss, SOAPdenovo-Trans, rnaSpades, Velvet/Oases:  Transcriptome assemblers.\n",
    "    * EvidentialGene: Transcriptome integration and reduction.\n",
    "    * rnaQuast: Transcriptome assessment.\n",
    "    * BUSCO: Assessment of completeness of the assembled transcriptome.\n",
    "    * DIAMOND/BLAST: Protein alignment for annotation.\n",
    "    * HMMER/Pfam: Protein domain assignment for annotation.\n",
    "    * Bowtie2: Read mapping for assembly validation.\n",
    "    * TransDecoder: ORF prediction and coding region identification.\n",
    "    * Trinotate: Functional annotation of transcripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0f4f2-5559-4675-9e97-24b0548b31af",
   "metadata": {},
   "source": [
    "## Get Started \n",
    "\n",
    "**Step 1:** Make sure you are in the correct local working directory as in `01_prog_setup.ipynb`.\n",
    "> It should be `/home/jupyter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec368bd6-699b-4dfb-8a4c-e20dc42a3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa66918-d431-46da-8922-3d6659b8e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c3682-deae-4eb3-a10b-c9c2be24852d",
   "metadata": {},
   "source": [
    "**Step 3A:** First, check the listings within the `resources directory`. Make sure you see the items listed below:\n",
    "```\n",
    "DBs  bin  conf  seq2  trans\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc95c9c-d19a-479c-8aa3-101191a00c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276e979-b4eb-40f0-b336-c41d3391549a",
   "metadata": {},
   "source": [
    "**Step 3B** Now, check the listing of the sequence directory: `seq2`. You should see seven pairs of gzipped fastq files (signified by the paired `.fastq.gz` naming). Six of these are for individual samples, and the seventh set, labeled **joined** is a concatenation of all files. Because of the way that TransPi works (as well as some of the programs that it uses), it's best to use a joined set of all sequences to make a unified transcriptome assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba183a-0e68-4e23-9f98-1d18dec69aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./resources/seq2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494cca1-3e15-4a27-bef7-bdeb9222a8da",
   "metadata": {},
   "source": [
    "**Step 4:** Now we are set to perform the assembly using the sequences within the directory `seq2/`.  \n",
    "> The specific sequences here are from zebrafish, and they represent a selected subset of the sequences from the experiment of [Hartig et al](https://journals.biologists.com/bio/article-pdf/5/8/1134/1114440/bio020065.pdf).\n",
    "\n",
    "The data was selected in order to create a reasonably large assembly (targeting a few hundred transcripts), while also being able to be checked against the \"known\" transcripts and genes).\n",
    "\n",
    "We will set only a small number of the options used in TransPi, focusing on the following:\n",
    "- `-profile docker`: This is a key setting, as it allows all software to be run from Docker container images, negating the need to install all programs locally (in other scenarios, there is the option to add more than one profile).\n",
    "    - The profile names are pointing to pre-defined groupings of setting within the `nextflow.config` file. \n",
    "- `--k 17,25,43`: The size(s) of *k*-mers to be used in the generation of the de Bruijn graphs (see the background file for a discussion of the role of *k*, and why it needs to be variable).\n",
    "- `--maxReadLen 50`: The maximum length of the reads (since these files all come from one experiment, this represents the length of all sequences).\n",
    "- `--all`: This setting tells Nextflow to run all steps from pre-assembly QC, through assembly and refinement, and then finally the analysis and tabulation of annotations to the putative transcripts.\n",
    "\n",
    "Under the assumption of an n1-high-memory node with 16 processors and 104GB of RAM, this run should take approximately **58 minutes**.\n",
    "\n",
    "As the workflow executes, the Nextflow engine will generate a directory called `work` where it places all of the intermediate information and output that is needed to carry out the work.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-lightbulb-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Tip: </b> Run-Time Reminder\n",
    "</div>\n",
    "\n",
    "\n",
    "> <img src=\"../images/jupyterRuntime.png\" width=\"500\">\n",
    ">\n",
    "> Remember that you can tell if the cell is still running by referring to the contents inside the `[ ]:` that sits to the left of the code cell. Or you can check the top right of the screen for the circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5985e3-93df-4779-afe1-4464e13bf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nextflow run main.nf --input test_samplesheet.csv -profile test,docker -c conf/test_params.config --run_mode full "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117d994-0502-4a58-b07a-861d254f11e2",
   "metadata": {},
   "source": [
    "The beauty and power of using a defined workflow in a management system (such as Nextflow) are that we not only get a defined set of steps that are carried out in the proper order, but we also get a well-structured and concise directory structure that holds all pertinent output (with naming specified in the command-line call of Nextflow and TransPi).\n",
    "\n",
    "**Step 5:** With the execution complete, let's look at what we have generated, first in the results directory. We will add the `-l` argument for a \"long listing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5a924-fbfc-49a9-ad8e-db707bca1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./basicRun/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d927ba-6916-47b5-8edf-1cc58d585f78",
   "metadata": {},
   "source": [
    "## Investigation and Exploration: Assembly and Annotation Results\n",
    "The use of an established and complex multi-step workflow (such as the TransPi workflow that you just ran) has the benefit of saving you a lot of manual effort in setting up and carrying out the individual steps yourself. It also is highly reproducible, given the same input data and parameters.\n",
    "\n",
    "It does, however, generate a lot of output, and it is beyond the scope of this training exercise to go through all of it in detail. We recommend that you download the complete results directory onto another machine or storage so that you can view it at your convenience, and on a less expensive machine than you are using to run this tutorial. *If you would like the proceed with the data in its current location, this also works, just bear in mind that it will cost roughly $0.72 per hour.*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  To Download...\n",
    "</div>\n",
    "\n",
    ">Here are two possible options to access the results files outside of this expensive JupyterLab instance.  \n",
    ">- If you instead have an external machine that accepts ssh connections, then you can use the secure copy scp command: `!scp -r ./basicRun/output/YOUR_USERID@YOUR.MACHINE`\n",
    ">- If you have a Google Cloud Storage bucket available, you can use the gsutil command: `!gsutil -m cp -r ./basicRun/output gs://YOUR-BUCKET-NAME-HERE/transpi-basicrun-output` to place all of your results into that bucket. \n",
    ">    - From there you have two options: \n",
    ">         1. (Recommended) You could create a new (cheaper) Vertex AI instance (or use an old one) and copy the files down into that new directory using the following gsutil command:`!gsutil -m cp -r gs://YOUR-BUCKET-NAME-HERE/transpi-basicrun-output ./`\n",
    ">         2. You could navigate to the bucket through the Google Cloud console and open the files through the links labeled `Authenticated URL`\n",
    ">\n",
    ">**In all of the commands above, you will need to edit the All-Caps part to match your own bucket or machine.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-lightbulb-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Tip: </b>\n",
    "</div>\n",
    "\n",
    "> - **After you have the output directory in its desired location, consider the information in the cell below as you explore the output.**\n",
    "> - **If you are viewing the output in a different location, consider copying or taking a screenshot of the cell below.**\n",
    "> - **Make sure that if you are viewing your output in a different location, you save your notebooks here, and then stop the VM instance, or it will keep costing money.**\n",
    "> - **Upon completion of your exploration, return to this submodule to complete the checkpoint quiz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c027e-edaf-4f96-9502-c4e4cfd67471",
   "metadata": {},
   "source": [
    "## Output Overview\n",
    "*These sub-directories will be mentioned in the order of their execution within TransPi.*\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  HTML files\n",
    "</div>\n",
    "\n",
    ">**If you are viewing your output within a JupyterLab VM instance, for the `.html` files to work correctly, you will need to select `Trust HTML` at the top of the screen.** This is due to the dynamic elements within the files.\n",
    "\n",
    "### FastQC\n",
    "> FastQC takes the raw read files and runs a swift analysis of the read data. The two key output files are `joined_R1_fastqc.html` and `joined_R2_fastqc.html` which provide a visual illustration of the read quality metrics. It is important to note that FastQC does not manipulate the data for further steps, it just outputs an analysis of the data quality.\n",
    "\n",
    "### Filter\n",
    "> FastP is a bioinformatics tool that preprocesses the raw read data. It trims poor-quality reads, removes adapter sequences, and corrects errors noticed within the reads. The `joined.fastp.html` provides an overview of the processing done on both read files.\n",
    "\n",
    "### Assemblies\n",
    "> TransPi uses five different assembly tools. All of the assembly `.fa` files are placed within the assemblies directory. For all of the assemblies except for Trinity, there are four `.fa` files: one for each of the *k*-mer length plus a compilation of all three. Trinity does not have the option to customize the *k*-mer size. Instead, it runs at a default `k=25`, therefore only having one assembly.\n",
    "\n",
    "### EviGene\n",
    "> At this point, we have a major overassembly of the transcriptome. We use a small piece of the EvidentialGene (EviGene) program known as tr2aacds which takes all of the assemblies and crunches them into a single, unified transcriptome. Within the evigene directory, there are two files: `joined.combined.fa` is all of the assemblies placed into the same file and`joined.combined.okay.fa` is the combined transcriptome after EviGene has reduced it down. In each header line, there is key information about the sequence.\n",
    ">> For example: `>SOAP.k17.C9429 58.0 evgclass=main,okay,match:SPADES.k43.NODE_313_length_1670_cov_12.047941_g161_i0,pct:100/100/.; aalen=392,75%,complete;`\n",
    ">>\n",
    ">> - This header indicates that this sequence was found in both the SOAP and SPADES assemblies.\n",
    ">> - The `eviclass=main` means that this sequence is the primary transcript, and there are alternates identified.\n",
    ">> - The `aalen=392` is the amino acid length of the sequence.\n",
    ">> - The `complete` means that it is a complete reading frame.\n",
    ">> - For more information on interpreting the headers from EviGene, reference the following [link](http://arthropods.eugenes.org/EvidentialGene/evigene/) in section 3.\n",
    "\n",
    "### BUSCO\n",
    "> BUSCO uses a database of known universal single-copy orthologs under a specific lineage (vertebrata in this case) and checks our assembled transcriptome for those sequences which it expects to find. BUSCO was run on both the TransPi assembly along with the assembly just done by Trinity. To visualize BUSCO's results, refer to the `short_summary.specific.vertebrata_odb10.joined.TransPi.bus4.txt` and `short_summary.specific.vertebrata_odb10.joined.Trinity.bus4.txt` files.\n",
    "\n",
    "### Mapping \n",
    "> One way to verify the quality of the assembly is to map the original input reads to the assembly (using an alignment program called bowtie2). There are two output files, one for the TransPi assembly and one for the Trinity exclusive assembly. These files are named `log_joined.combined.okay.fa.txt` and `log_joined.Trinity.fa.txt`.\n",
    "\n",
    "### rnaQUAST\n",
    "> rnaQUAST is another assembly assessment program. It provides statistics about the transcripts that have been produced. For a brief overview of the transcript statistics, refer to `joined_rnaQUAST.csv`.\n",
    "\n",
    "### TransDecoder \n",
    "> TransDecoder is a program that \"decodes\" the transcripts. First, it identifies open reading frames (ORFs). From there, it then will make predictions on what is likely to be coding regions. For statistics regarding TransDecoder, refer to the `joined_transdecoder.stats` file.\n",
    "\n",
    "### Trinotate\n",
    "> Trinotate uses the information regarding likely coding regions produced by TransDecoder to make predictions about potential protein function. It does this by cross-referencing the assembled transcripts to various databases such as pfam and hmmer. These annotations can be viewed in the `joined.trinotate_annotation_report.xls` file.\n",
    "\n",
    "### Report\n",
    "> Within `report` is one file: `TransPi_Report_joined.html`. This is an HTML file that combines the results throughout TransPi into a series of visual tables and figures.\n",
    ">> The sub-directories `stats` and `figures` are intermediary sub-directories that hold information to generate the report.\n",
    "\n",
    "### pipeline_info\n",
    "> One of the benefits of using Nexflow and a well-defined pipeline/workflow is that when the run is completed, we get a high-level summary of the execution timeline and resources. Two key files within this sub-directory are `transpi_timeline.html` and `transpi_report.html`. In the `transpi_timeline.html` file, you can see a graphic representation of the total execution time of the entire workflow, along with where in the process each of the programs was active. From this diagram, you can also infer the ***dependency*** ordering that is encoded into the TransPi workflow. For example, none of the assembly runs started until the process labeled **`normalize reads`** was complete because each of these is run on the normalized data, rather than the raw input. Similarly, **`evigene`**, the program that integrates and refines the output of all of the assembly runs doesn't start until all of the assembly processes are complete. Within the `transpi_report.html` file, you can get a view of the resources used and activities carried out by each process, including CPUs, RAM, input/output, container used, and more.\n",
    "\n",
    "### RUN-INFO.txt\n",
    "> `RUN-INFO.txt` provides the specific details of the run such as where the directories are and the versions of the various programs used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372f902-6138-4217-86a8-4b0002f5f387",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "    <b>Checkpoint 1:</b>\n",
    "</div>\n",
    "\n",
    "*The green cards below are interactive. Spend some time to consider the question and click on the card to check your answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e23c5b-a6e7-4ad7-b69c-7dc42fa9bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytercards import display_flashcards\n",
    "display_flashcards('../quiz-material/02-cp1-1.json')\n",
    "display_flashcards('../quiz-material/02-cp1-2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f0b3a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This Jupyter Notebook demonstrated a complete transcriptome assembly workflow using the TransPi Nextflow pipeline.  We successfully executed the pipeline, encompassing quality control, normalization, multiple assembly generation with Trinity, TransAbyss, SOAP, rnaSpades, and Velvet/Oases, integration via EvidentialGene, and subsequent assessment using rnaQuast and BUSCO.  The final assembly underwent annotation with DIAMOND/BLAST and HMMER/Pfam, culminating in comprehensive reports detailing the entire process and the resulting transcriptome characteristics.  The generated output, accessible in the `basicRun/output` directory, provides a rich dataset for further investigation and analysis, including detailed quality metrics, assembly statistics, and functional annotations.  This module provided a practical introduction to automated transcriptome assembly, highlighting the efficiency and reproducibility offered by integrated workflows like TransPi.  Further exploration of the detailed output is encouraged, and the subsequent notebook focuses on a more in-depth annotation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68484f3",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Remember to proceed to the next notebook [`Submodule_03_annotation_only.ipynb`](Submodule_03_annotation_only.ipynb) or shut down your instance if you are finished."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
