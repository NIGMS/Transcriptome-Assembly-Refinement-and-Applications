{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ea1e55-d687-4189-b90f-5ce35b3c89de",
   "metadata": {},
   "source": [
    "# MDIBL Transcriptome Assembly Learning Module\n",
    "# Notebook 2: Using \"denovoscript\" to Performing an \"Annotation Only\" Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36451c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This Jupyter Notebook provides a learning module on transcriptome assembly, specifically focusing on annotation using the `denovoscript` pipeline. It guides users through an \"annotation only\" run, assuming a pre-assembled transcriptome.  The notebook begins with an introductory video and illustration of the annotation workflow.  It then demonstrates downloading a rainbow trout transcriptome from an Amazon S3 bucket and counting its sequences.  Users are instructed to set up AWS Batch for serverless Nextflow execution, either automatically via a CloudFormation template or manually. After installing Nextflow and switching the kernel, `denovoscript` is executed in annotation-only mode using the downloaded transcriptome.  Results are then downloaded from S3 to the local directory for inspection. The notebook then introduces the concept of Docker containers and guides users through running BUSCO within a container to assess transcriptome completeness using the vertebrata gene set.  Finally, interactive quizzes prompt users to interpret BUSCO, GO, and TransDecoder results, emphasizing the importance of understanding data provenance.  A second, user-driven BUSCO analysis on a different transcriptome is assigned as a final exercise, encouraging exploration of different lineages and critical evaluation of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbab1d",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* **Understanding transcriptome annotation:** Learn the process of annotating a pre-assembled transcriptome.\n",
    "* **Using `denovoscript` for annotation:**  Gain practical experience using the `denovoscript` pipeline with the `annotation_only` run mode.\n",
    "* **Working with AWS Batch:** Learn how to set up and utilize AWS Batch for running Nextflow pipelines in a serverless environment.\n",
    "* **Understanding and using Docker containers:**  Become familiar with Docker containers and how to execute bioinformatics tools like BUSCO within them.\n",
    "* **Assessing transcriptome completeness with BUSCO:** Learn how to use BUSCO to evaluate the completeness of a transcriptome assembly using different lineage datasets.\n",
    "* **Interpreting BUSCO, GO, and TransDecoder results:** Develop skills in interpreting the output files generated by these tools and understanding their implications.\n",
    "* **Understanding data provenance:** Appreciate the importance of considering the origin and processing of transcriptomic data before analysis.\n",
    "* **Running BUSCO analysis independently:**  Apply learned concepts by independently constructing and executing BUSCO commands for different transcriptomes and lineages.\n",
    "* **Critical evaluation of BUSCO results:** Learn to analyze BUSCO results critically, considering factors such as transcriptome quality, lineage selection, and biological explanations for observed patterns (e.g., duplicated or fragmented genes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3bb78",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**1. Software/Environment:**\n",
    "\n",
    "*   Jupyter Notebook (Python kernel)\n",
    "*   AWS CLI (configured)\n",
    "*   Nextflow (installed via `mamba`, switch kernel to `conda_nextflow`)\n",
    "*   Docker (running, user permissions correct)\n",
    "*   `jupytercards` (install via `pip`)\n",
    "\n",
    "**2. Enabled APIs:**\n",
    "\n",
    "*   AWS Batch\n",
    "*   Amazon S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adea33",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81dff4-64b8-42ca-91b5-6ad4cef392ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the command below to watch the video\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('AGuUHmSobEA', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cd97f-fc62-40ab-badc-ba0bc6f092e4",
   "metadata": {},
   "source": [
    "> <img src=\"../images/AnnotationProcess.png\" width=\"800\">\n",
    ">\n",
    "> **Figure 1:** Annotation workflow for a new, unannotated transcriptome. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51452ca0",
   "metadata": {},
   "source": [
    "### **Step 1:** Count the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6b740-f02c-4993-b082-8cb58fc46e7b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  The Transcriptome\n",
    "</div>\n",
    "\n",
    "> The transcriptome that we are using will be downloaded onto from a public Amazon S3 bucket your local directory. It lives within the `resources` directory in the sub-directory named `trans`. It is in the file format `.fa`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042b6ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  aws s3\n",
    "</div>\n",
    "\n",
    ">`aws s3` is a tool allows you to interact with Amazon S3 buckes through the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc86e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp --recursive s3://nigms-sandbox/nosi-inbremaine-storage/resources ./resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755b541-007b-4773-a53e-f01eee1ed570",
   "metadata": {},
   "source": [
    "> You should get a count of 31,176."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db05d70-513c-4ef4-92d6-8253562b43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -c \">\" ./resources/trans/Oncorhynchus_mykiss_GGBN01.1.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065fc1a-0a64-447c-8628-b20d5277dd3b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  What is the Oncorhynchus mykiss?\n",
    "</div>\n",
    "\n",
    "> The Oncorhynchus mykiss is commonly known as the **Rainbow Trout**. Here is what they look like:\n",
    ">\n",
    "> <img src=\"../images/rainbowTrout.jpeg\"  width=\"500\" >\n",
    "> \n",
    ">> Image Source: https://www.ndow.org/species/rainbow-trout/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64228197",
   "metadata": {},
   "source": [
    "### **Step 2:** AWS Batch Setup\n",
    "\n",
    "AWS Batch will create the needed permissions, roles and resources to run Nextflow in a serverless manner. You can set up AWS Batch manually or deploy it **automatically** with a stack template. The Launch Stack button below will take you to the cloud formation create stack webpage with the template with required resources already linked. \n",
    "\n",
    "If you prefer to skip manual deployment and deploy automatically in the cloud, click the Launch Stack button below. For a walkthrough of the screens during automatic deployment please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/HowToLaunchAWSBatch.md). The deployment should take ~5 min and then the resources will be ready for use. \n",
    "\n",
    "[![Launch Stack](../images/LaunchStack.jpg)](https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=aws-batch-nigms&templateURL=https://nigms-sandbox.s3.us-east-1.amazonaws.com/cf-templates/AWSBatch_template.yaml)\n",
    "\n",
    "\n",
    "Before beginning this tutorial, if you do not have required roles, policies, permissions or compute environment and would like to **manually** set those up please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/AWS-Batch-Setup.md) to set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a617",
   "metadata": {},
   "source": [
    "#### Change the parameters as desired in `aws` profile inside `../denovotrascript/nextflow.config` file:\n",
    " - Name of your **AWS Batch Job Queue**\n",
    " - AWS region \n",
    " - Nextflow work directory\n",
    " - Nextflow output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb13bb",
   "metadata": {},
   "source": [
    "### **Step 3:** Install Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa49095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! mamba create  -n nextflow -c bioconda nextflow -y\n",
    "! mamba install -n nextflow ipykernel -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b76d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "    <b>Alert: </b> Remember to change your kernel to <b>conda_nextflow</b> to run nextflow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d1b9b",
   "metadata": {},
   "source": [
    "### **Step 4:** Run `denovotranscript`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1541b9-abb6-47c0-aa49-5c1720680376",
   "metadata": {},
   "source": [
    "Now we can run `denovotranscript` using the option `annotation_only` run-mode which assumes that the transcriptome has been generated, and will only run the various steps for annotation of the transcripts.\n",
    "\n",
    ">This run should take about **5 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb702b3-0a36-4a32-aca5-50dc75f9d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run ../denovotranscript/main.nf --input ../denovotranscript/test_samplesheet_aws.csv -profile aws \\\n",
    "--run_mode annotation_only --transcript_fasta s3://nigms-sandbox/nosi-inbremaine-storage/resources/trans/Oncorhynchus_mykiss_GGBN01.1.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f8dfb-366d-4e0f-af4e-d96f6ee97d34",
   "metadata": {},
   "source": [
    "The output will be arranged in a directory structure in your Amazon S3 bucket. We will download it into our local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458f219-af9a-47d5-bb6d-300399801319",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p <Your-Output-Directory-annotation-only>\n",
    "! aws s3 cp --recursive s3://<YOUR-BUCKET-NAME>/<Your-Output-Directory-annotation-only>/ ./<Your-Output-Directory-annotation-only>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a199f-f19b-42a8-b63d-ca2a4b8e35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l ./<Your-Output-Directory-annotation-only>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ac17d",
   "metadata": {},
   "source": [
    "----\n",
    "# Andrea, please update this part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b1049",
   "metadata": {},
   "source": [
    "Let's take a look at the `RUN_INFO.txt` file to see what the parameters and programs associated with our analysis were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ee1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./onlyAnnRun/output/RUN_INFO.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df312985",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187a790-276c-4bf2-8ce8-2f7985e8c662",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  Containers\n",
    "</div>\n",
    "\n",
    ">Note that while the \"annotation_only\" run-mode carries out the searches against Pfam and the BLAS analysis against known proteins, it does not carry out the BUSCO analysis. We can make that happen ourselves however, to do so, we need to learn a little bit about running programs from containers.\n",
    ">\n",
    ">Container systems (and associated images) are one approach that simplifies the use of a broad set of programs, such as is commonly found in the wide field of computational biology. To put it concisely, most programs are not \"stand-alone\" but instead rely upon at least a few supporting libraries or auxiliary programs.  Since many analyses require multiple programs, installation of the necessary programs will also require installation of the supporting components, and critically *sometimes the supporting components of one program conflict with those of other programs.* \n",
    ">\n",
    ">Container systems ([Docker](https://www.docker.com/) and [Singularity](https://sylabs.io/singularity/) are the two most well-known examples) address this by installing and encapsulating the program and all of its necessary supporting components in an image. Each program is then executed in the context of its container image, which is activated just long enough to run its program.\n",
    ">\n",
    ">Because of the way that we have run the TransPi workflow in the previous, our system will already have several container images installed. We can now work directly with these images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fed5f-0d41-4fc3-8a7c-2600e2993490",
   "metadata": {},
   "source": [
    "### **Step 5:** Activate the BUSCO container\n",
    ">We want the Docker image that contains the program (and all necessary infrastructure) for running the BUSCO analysis. The name is in the first column, but we also need the version number, which is in the second column. So let's put that together and first activate the container and ask it to run BUSCO and just give us back the help message.\n",
    ">\n",
    ">We will use the `docker run` command, and we will use the following options with it:\n",
    ">- `-it`, which means run interactively\n",
    ">- `--rm`, which means clean up after shutting down\n",
    ">- `--volume /home:/home` This is critical because, by default, a Docker image can only see the file system inside of the container image. We need to have it see our working directory, so we create a volume mapping. For simplicity, we will just map the /home directory outside the container to the same address inside. This will let us access and use all of the files that are below `/home`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9539e-5f1f-4590-9b1b-1a20231acd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --volume /home:/home quay.io/biocontainers/busco:5.4.3--pyhdfd78af_0 busco --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4be7c",
   "metadata": {},
   "source": [
    "Get a listing of the images that are currently loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ac358-15d1-4d97-99db-b0d4a7a6ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282b861-9616-4c07-98cf-bbe0b96747c1",
   "metadata": {},
   "source": [
    "### **Step 6:** Run BUSCO (in the container)\n",
    ">Now we will fill out a complete command and ask BUSCO to analyze the same trout data that we just used above. Here is the full command needed to make this run go. A lot is going on here:\n",
    ">\n",
    ">- `-i /home/jupyter/resources/trans/Oncorhynchus_mykiss_GGBN01.1.fa`: this points to the location and name of the file to be examined.\n",
    ">- `-l vertebrata_odb10`: this tells BUSCO to use the vertebrata gene set (genes common to vertebrates) as the target.\n",
    ">- `-o GGBN01_busco_vertebrata`: this tells BUSCO to use this as the label for the output.\n",
    ">- `--out_path /home/jupyter/buscoOutput`: this tells BUSCO where to put the output directory. \n",
    ">- `-m tran`: this tells BUSCO that the inputs are transcripts (rather than protein or genomic data). \n",
    ">- `-c 14`: this tells BUSCO to use 14 CPUs\n",
    ">\n",
    "> This should take about **an hour**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d3a6b-6d07-4483-8a57-6d795c4eb602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the command below to watch the video\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('D95mFnIjRo4', width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82296cfb-cddb-4325-a8a6-ab3eea8fdfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numthreads=!lscpu | grep '^CPU(s)'| awk '{print $2-1}'\n",
    "THREADS = int(numthreads[0])\n",
    "! echo $THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d031c6-6a25-46fc-b041-e69e92fdb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --volume /home:/home quay.io/biocontainers/busco:5.4.3--pyhdfd78af_0 busco \\\n",
    "-i /home/ec2-user/SageMaker/Transcriptome-Assembly-Refinement-and-Applications/AWS/resources/trans/Oncorhynchus_mykiss_GGBN01.1.fa \\\n",
    "-l vertebrata_odb10 -o GGBN01_busco_vertebrata \\\n",
    "--out_path /home/ec2-user/SageMaker/Transcriptome-Assembly-Refinement-and-Applications/AWS/buscoOutput -m tran -c $THREADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3d8cb-2cb2-4522-89b6-4f102ad4658f",
   "metadata": {},
   "source": [
    "Look at the output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbb15e-b08b-48a6-8acc-f450fc98152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./buscoOutput/GGBN01_busco_vertebrata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9da3fe-ca2c-41f7-9af9-7384c5688a47",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "    <b>Checkpoint 1:</b> Interpret The Results \n",
    "</div>\n",
    "\n",
    "> Consider the following result files:\n",
    "> - The BUSCO result `./buscoOutput/GGBN01_busco_vertebrata/short_summary.specific.vertebrata_odb10.GGBN01_busco_vertebrata.txt`\n",
    "> - The GO stats result `./onlyAnnRun/output/stats/Oncorhynchus_mykiss_GGBN01.sum_GO.txt`\n",
    "> - The TransDecoder stats result: `./onlyAnnRun/output/stats/Oncorhynchus_mykiss_GGBN01.sum_transdecoder.txt`\n",
    "\n",
    "*The green cards below are interactive. Spend some time to consider the question and click on the card to check your answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07364c-507a-4408-a5c5-bd865869c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jupytercards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721adc8-08ff-4b35-b994-1d18e61e7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytercards import display_flashcards\n",
    "display_flashcards('../quiz-material/03-cp1-1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028ed80-7227-49ee-b4e1-c123ae5bdfda",
   "metadata": {},
   "source": [
    "> Now let's take a look at where the data came from... Consider the abstract of the [Al-Tobasel et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4764514/) paper published from this data.\n",
    ">\n",
    ">>*The ENCODE project revealed that ~70% of the human genome is transcribed. While only 1–2% of the RNAs encode for proteins, the rest are non-coding RNAs. Long non-coding RNAs (lncRNAs) form a diverse class of non-coding RNAs that are longer than 200nt. Emerging evidence indicates that lncRNAs play critical roles in various cellular processes including regulation of gene expression. LncRNAs show low levels of gene expression and sequence conservation, which make their computational identification in genomes difficult. In this study, more than two billion Illumina sequence reads were mapped to the genome reference using the TopHat and Cufflinks software. Transcripts shorter than 200nt, with more than 83–100 amino acids ORF, or with significant homologies to the NCBI nr-protein database were removed. In addition, a computational pipeline was used to filter the remaining transcripts based on a protein-coding-score test. Depending on the filtering stringency conditions, between 31,195 and 54,503 lncRNAs were identified, with only 421 matching known lncRNAs in other species. A digital gene expression atlas revealed 2,935 tissue-specific and 3,269 ubiquitously-expressed lncRNAs. This study annotates the lncRNA rainbow trout genome and provides a valuable resource for functional genomics research in salmonids.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654ccf7-de98-443a-9dd2-6563767a4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flashcards('../quiz-material/03-cp1-2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954beff-d647-4a1c-9b21-ad05e4fa16c3",
   "metadata": {},
   "source": [
    ">**The key takeaway is to always be mindful of the data you are using before performing analysis on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17554548-3d09-4984-8ebe-4a6b26a744a1",
   "metadata": {},
   "source": [
    "Now let's try with one of the other transcriptomes that we downloaded from the NCBI Transcriptome Shotgun Assembly archive.\n",
    "> This should take about **an hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684820c8-ee9f-495d-8476-9f25eaa02d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --volume /home:/home quay.io/biocontainers/busco:5.4.3--pyhdfd78af_0 busco \\\n",
    "    -i /home/ec2-user/SageMaker/Transcriptome-Assembly-Refinement-and-Applications/AWS/resources/trans/Pseudacris_regilla_GAEI01.1.fa \\\n",
    "    -l vertebrata_odb10 -o GAEI01_busco_vertebrata \\\n",
    "    --out_path /home/ec2-user/SageMaker/Transcriptome-Assembly-Refinement-and-Applications/AWS/buscoOutpu -m tran -c $THREADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b26cfb-9f5f-45a5-8466-7e94110c480d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "    <b>Checkpoint 2:</b> Your turn to run a BUSCO analysis \n",
    "</div>\n",
    "\n",
    ">For this checkpoint, you will run another BUSCO analysis, however, this time you will write your own execution command. For the transcriptome used, you have two options:\n",
    ">1. Within the directory that we have been using for the previous two BUSCO runs, `./resources/trans`, there is one more assembled transcriptome named `Microcaecilia_dermatophaga_GFOE01.1.fa`.\n",
    ">2. Go onto the NCBI Transcriptome Shotgun Assembly archive, find your own complete, assembled transcriptome, and use that.\n",
    ">    - If you download the file onto your local computer, there is an upload button (up arrow) in the top left of the Jupyter interface where you can upload the file.\n",
    ">    - If the file you have uploaded is zipped, you will need to unzip it using the following commands: (make sure that the file name after the `>` has the `.fa` extension.)\n",
    ">```python\n",
    "        !gzip -d -c ./PATH/TO/FILE.fsa_nt.gz > ./PATH/TO/FILE.1.fa\n",
    "        !rm ./PATH/TO/FILE.fsa_nt.gz\n",
    ">```\n",
    "> Additionally, consider trying a different lineage (`-l` selection). EZlab, the creators of BUSCO, have produced a large selection of lineages to choose from. Each one has a different set of genes that BUSCO looks for. If you decide to try a different lineage, it is recommended to choose a lineage that falls somewhere within the same family. (e.g., Don't choose the `primates_odb10` lineage if you are choosing to use a bullfrog transcriptome.)\n",
    ">```python\n",
    "        # This will be a complete list of the available datasets\n",
    "        !docker run -it --rm --volume /home:/home quay.io/biocontainers/busco:5.4.3--pyhdfd78af_0 busco --list-datasets\n",
    ">```\n",
    "> Feel free to reference the commands for the previous BUSCO runs and the help command we ran earlier if you are stuck. Additionally, feel free to check out the [BUSCO user guide](https://busco.ezlab.org/busco_userguide.html).\n",
    ">\n",
    ">After the run has been complete, consider the following:\n",
    ">1. How did BUSCO perform on this transcriptome? Does the transcriptome appear to be well assembled based on the provided lineage? If the results are not good, consider the possible reasons why? Is it more likely that the transcriptome chosen was not good? Or potentially a poorly chosen lineage? Or maybe something else entirely?\n",
    ">2. What could be a logical biological reason the output says that there are duplicate copies of the same gene?\n",
    ">3. What could be a possible reason for fragmented copies?\n",
    ">4. Why is it that broader lineages such as metazoa have far fewer genes (954) that BUSCO looks for compared to more specific lineages such as mammalia which has far more genes (9226) that BUSCO looks for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a46ef9-5459-4302-9a34-bf92528c9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your BUSCO command here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b88c4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a comprehensive hands-on experience in transcriptome annotation using the `denovoscript` pipeline in annotation-only mode, leveraging AWS Batch for serverless execution and Docker containers for BUSCO analysis. Through a guided workflow, users learned to set up AWS Batch, execute `denovoscript` to annotate a rainbow trout transcriptome, assess transcriptome completeness with BUSCO, and critically interpret the results from BUSCO, GO, and TransDecoder analyses. Furthermore, the notebook emphasized the importance of understanding data provenance and culminated in an independent BUSCO analysis exercise, challenging users to apply their newfound skills to different transcriptomes and critically evaluate the outcomes, thus solidifying their understanding of transcriptome assembly and annotation principles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc80021",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Remember to proceed to the next notebook [`Submodule_04_gls_assembly.ipynb`](Submodule_04_gls_assembly.ipynb) or shut down your instance if you are finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_nextflow",
   "language": "python",
   "name": "conda_nextflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
