{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481f3583-186d-4d0e-b2c8-0381b6cf814f",
   "metadata": {},
   "source": [
    "# MDIBL Transcriptome Assembly Learning Module\n",
    "# Notebook 4: Using TransPi on Google Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512337d-7ade-44c7-832a-ae6970a7d980",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "So far, all of the computational work executed has been run locally, using the compute resources available within this Jupyter notebook. Although this is functional, it is not the ideal setup for fast, cost-efficient data analysis.\n",
    "\n",
    "Google Batch is known as a scheduler, which provisions specific compute resources to be allocated for individual processes within our workflow. This provides two primary benefits:\n",
    "> - Once each specific process is complete, the computer will automatically turn off, meaning that you aren't wasting any money on unused resources.\n",
    "> - Multiple processes can be executed at the same time, allowing for the parallelization of computational tasks. This means that the computational process is quicker from start to finish.\n",
    "\n",
    "Fortunately, Batch and Nextflow are compatible with each other allowing for any Nextflow workflow, including the TransPi workflow that we have been using, to be executable on Batch.\n",
    "\n",
    "\n",
    "> <img src=\"images/gcbDiagram.jpg\" width=\"1200\">\n",
    ">\n",
    "> **Figure 1:** Diagram illustrating the interactions between the components used for the Google Batch run. \n",
    "\n",
    "For this to work, there are a few quick adjustment steps to make sure everything is set up for a Google Batch run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b495639",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "1. **Utilize Google Batch for efficient and cost-effective data analysis:**  The notebook contrasts local computation with Google Batch, highlighting the benefits of the latter in terms of cost savings (auto-shutdown of unused resources) and speed (parallelization of tasks).\n",
    "\n",
    "2. **Integrate Nextflow workflows with Google Batch:** The notebook demonstrates how to configure a Nextflow pipeline (TransPi) to run on Google Batch, emphasizing the compatibility between these tools.\n",
    "\n",
    "3. **Manage files using Google Cloud Storage (GCS):**  The lesson requires users to create or utilize a GCS bucket to store the necessary files for the TransPi workflow, addressing the challenge of accessing local files from external compute resources.\n",
    "\n",
    "4. **Configure a Nextflow pipeline for Google Batch execution:** This involves modifying the `nextflow.config` file to point to the GCS bucket, adjust compute allocations (CPU and memory), and specify the correct Google Batch profile.  It shows how to use Perl one-liners for efficient configuration changes.\n",
    "\n",
    "5. **Interpret and compare the timelines of local and Google Batch runs:**  By comparing the `transpi_timeline.html` files from both local and Google Batch executions, users learn to analyze the performance differences and understand the impact of resource allocation.\n",
    "\n",
    "6. **Execute and manage a Nextflow pipeline on Google Batch:** The notebook provides step-by-step instructions for running TransPi on Google Batch using specific command-line arguments and managing the output.\n",
    "\n",
    "7. **Understand and utilize Google Cloud commands:**  The notebook uses `gcloud` and `gsutil` commands extensively, teaching users basic Google Cloud command-line interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd972f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* **A Google Cloud Storage (GCS) Bucket:**  A bucket is needed to store the TransPi workflow's input files and output results. The notebook provides options to create a new bucket or use an existing one.\n",
    "* **Sufficient Compute Resources:** The user needs to have sufficient quota available in their GCP project to handle the compute resources required by the TransPi workflow (CPUs, memory, disk space).  The notebook uses a `nextflow.config` file to configure the Google Batch execution.\n",
    "* **`gcloud` CLI:** The Google Cloud SDK (`gcloud`) command-line tool must be installed and configured to authenticate with the GCP project.  The notebook uses `gcloud` commands to interact with GCP services.\n",
    "* **`gsutil` CLI:** The `gsutil` command-line tool (part of the Google Cloud SDK) is used to interact with GCS.\n",
    "* **Nextflow:** The Nextflow workflow engine must be installed locally on the Jupyter Notebook environment.\n",
    "* **TransPi Workflow:** The TransPi Nextflow pipeline code must be available in the Jupyter Notebook environment's file system.  The notebook assumes it's in a `TransPi` directory.\n",
    "* **Perl:** The notebook uses Perl one-liners for file manipulation.  Perl must be installed in the Jupyter Notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449ee77",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a81048-5b92-4ee4-9ede-28abe3ccf5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the command below to watch the video\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('abw2XAg1e_g', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14495602-64ba-44ac-9ee7-478709cee34c",
   "metadata": {},
   "source": [
    "**Step 1:** Downsize the VM instance.\n",
    "> Consider downloading or taking a screenshot of the following image as the downsizing process will involve briefly stopping this VM instance.\n",
    ">\n",
    "> <img src=\"images/VMdownsize.jpg\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab5976-4523-4816-85b7-bcd748feb6ec",
   "metadata": {},
   "source": [
    "**Step 2:** Once again we are going to set the local working directory back to `/home/jupyter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca889ac-dcdc-46f3-abd3-01c9d9ff3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537685b-794e-496f-97b9-a54694918bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be921e6-dd86-48d9-a8cc-d7baa5e99d08",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  Bucket for Batch\n",
    "</div>\n",
    "\n",
    "> Batch is using external machines to do our computing work for us, which means that it is unable to find files that we have locally within this Jupyter notebook. As a result, we need to put the files that TransPi needs to run in a location that is findable from these machines: Google Cloud Storage (GCS) buckets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da11b26-3914-49dd-8a8b-f796ff66626c",
   "metadata": {},
   "source": [
    "**Step 3:** Create a variable for your Google project name\n",
    "> - The first line is a Google Cloud command that gets the name of your project and puts it in a list named projName.\n",
    "> - The second line gets the name, which is at the 0 index of the list and sets it to the variable `myProject`.\n",
    "> - The third line just prints out the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5b0d0-3634-4baa-b7c5-b37a111edb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "projName=!gcloud config get-value project\n",
    "myProject=projName[0]\n",
    "myProject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8628b174-a2e4-4585-b332-0bd04c4df6f0",
   "metadata": {},
   "source": [
    "**Step 4a:** Bucket Setup:\n",
    "\n",
    "Set the variable `myBucketName` to one of the following:\n",
    "1. If you plan on using an existing bucket, then set it to the name of that bucket.\n",
    "2. If you would like to use a new bucket, then set the variable to whatever you would like to name your new bucket. Here are some quick naming guidelines:\n",
    "    - You can use lowercase letters, numbers, dashes, underscores, and dots. \n",
    "    - The name cannot start or end in a dash, underscore, or dot.\n",
    "    - Keep the name within the quotes.\n",
    "    - More information can be found [here](https://cloud.google.com/storage/docs/buckets?_ga=2.188214954.-360038957.1673379287#naming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ba2f4-c75d-4cc0-af5e-9432919b3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myBucketName=\"your-bucket-name\"\n",
    "myBucketName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9985d",
   "metadata": {},
   "source": [
    "**Step 4b:** Replace names in files with personal bucket and project names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i \"s/<YOUR-PROJECT-ID>/$myProject/g\" nextflow.config\n",
    "! sed -i \"s/<YOUR-BUCKET-NAME>/$myBucketName/g\" conf/test_params.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676e5d2-f556-463e-93c2-5add30f1fff8",
   "metadata": {},
   "source": [
    "**Step 4c:** Create a new GCS bucket. *If you are using an existing bucket, you can skip this step.*\n",
    "> To do this, we can use a new `gsutil` command: `mb` which stands for make bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dbf3b-14d7-43ec-9253-2698219c2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -p $myProject -c STANDARD -b on gs://$myBucketName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3b453-4692-4db3-86ab-c6234244888f",
   "metadata": {},
   "source": [
    "**Step 5:** Create a Google-recognizable path variable named `gbPath`.\n",
    "> You don't need to change anything, just execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b3e78-bb3c-4072-89da-41c622537dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbPath=\"gs://\" + myBucketName + \"/TransPi\"\n",
    "gbPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37655a-3d46-4f53-829c-16996e2d1751",
   "metadata": {},
   "source": [
    "**Step 6:** Copy the `resources` directory into your bucket.\n",
    "> These are the same resources that we copied to the local directory in Submodule 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39277a-1128-4b7b-a5a2-db8491bc89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://nigms-sandbox/nosi-inbremaine-storage/resources $gbPath/resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f33dee-6928-41ad-84b9-da1eaa3823a3",
   "metadata": {},
   "source": [
    "**Step 7A:** Adjust our `nextflow.config` file paths.\n",
    "\n",
    "This changes all of the pointers to our resources in the GCS bucket.\n",
    "> This is a Perl one-liner that is very similar to the one used in Submodule 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db418c-ebe3-409c-9ef9-70205e856089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl -i.annloc -pe s#/home/jupyter#$gbPath#g ./TransPi/nextflow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaab62-344f-443b-99e0-345e04ead1e2",
   "metadata": {},
   "source": [
    "**Step 7B:** Adjust the names of directories and add your project name to the gcb profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41b8cf-5d55-4bd2-86cc-afb24c6e492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl -i -pe 's/onlyAnnRun/basicRun/g' ./TransPi/nextflow.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e16dc8-79ff-48f2-99f4-e6a7ed8bb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl -i -pe s#your-project-name#$myProject#g ./TransPi/nextflow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2417a7-6f8b-4ea6-aefb-3c36cc0d604a",
   "metadata": {},
   "source": [
    "**Step 7C:** Adjust our `nextflow.config` compute allocations.\n",
    "\n",
    "Now that we are using separately provisioned compute resources, we can allocate more CPU power and memory to specific processes.\n",
    "\n",
    "> These are also Perl one-liners, but this time they are delimited with `/` instead of `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbd33e-cb1c-4eda-b82f-92bcc00aec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl -i -pe \"s/cpus='15'/cpus='20'/g\" ./TransPi/nextflow.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42741c52-0e12-49c9-a802-40f5e41441f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl -i -pe \"s/memory='100 GB'/memory={ 100.Gb + (task.attempt * 50.Gb)}/g\" ./TransPi/nextflow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa334e6-f3bf-4c4a-85ae-c6e1cc2e6060",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "    <b>Checkpoint 1:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80b91f-f798-4cea-956b-17c406aaf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytercards import display_flashcards\n",
    "display_flashcards('Transcriptome-Assembly-Refinement-and-Applications/quiz-material/04-cp1-1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f0fa9-3efa-468d-9c2b-7e6c23bec0d0",
   "metadata": {},
   "source": [
    "**Step 8:** Time to run TransPi using Batch.\n",
    "> This should take about **40 minutes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd17b0d-683d-4a02-a347-26b65fddacbb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <i class=\"fa fa-hand-paper-o\" aria-hidden=\"true\"></i>\n",
    "    <b>Note: </b>  gcb profile\n",
    "</div>\n",
    "\n",
    "> Note that in the command, we use the profile gcb. This tells Nextflow that we want to use the gcb profile designated within the `nextflow.config` file. Here is what that profile looks like: \n",
    ">```\n",
    "    gcb {\n",
    "\tprocess.executor = 'google-batch'\n",
    "    process.container = 'ubuntu'\n",
    "    google.location = 'us-central1'\n",
    "    google.region = 'us-central1'\n",
    "    google.project = 'your-project-name'\n",
    "    workDir = 'gs://your-bucket-name/TransPi/basicRun/work'\n",
    "    params.outdir='gs://your-bucket-name/TransPi/basicRun/output'\n",
    "    google.batch.bootDiskSize=50.GB\n",
    "    google.storage.parallelThreadCount = 100\n",
    "    google.storage.maxParallelTransfers = 100\n",
    "    }\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b9f7a-d069-444b-894a-5b974c2bb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!NXF_VER=22.10.1 ./nextflow run ./TransPi/TransPi.nf \\\n",
    "-profile gcb --k 17,25,43 --maxReadLen 50 --all -resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7b532-5ff8-451f-8cc3-01eb827133fd",
   "metadata": {},
   "source": [
    "**Step 9:** Take a look at `transpi_timeline.html` and compare it to the timeline of the local run.\n",
    "\n",
    ">First we have to make a local directory to place the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2188706-1cbe-4555-8294-96e897999fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir GCBbasicRun\n",
    "!mkdir ./GCBbasicRun/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11c423-042b-42ca-b46f-757945d27257",
   "metadata": {},
   "source": [
    ">Now we can copy over the `pipeline_info` from the bucket to our new local bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8f4eb-0e3c-45ce-a5b8-042d66592fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r $gbPath/basicRun/output/pipeline_info ./GCBbasicRun/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd2852-d47d-498a-b480-c70074c94f42",
   "metadata": {},
   "source": [
    ">Now we can visualize both the local and GCB run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b61178-8282-442e-bd05-e72d11b05ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../GCBbasicRun/output/pipeline_info/transpi_timeline.html\",width=1200, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15338b8b-fdc6-43ce-8140-a5011a328451",
   "metadata": {},
   "source": [
    "> **Figure 1:** GCB Run Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d2ac1-46cb-421e-b6bd-2394d831ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame('../basicRun/output/pipeline_info/transpi_timeline.html',width=1200, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b514636-6ab2-47cc-89fd-61e4a71a8ed4",
   "metadata": {},
   "source": [
    "> **Figure 2:** Local Run Timeline Above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb6344-3e3f-4a4c-ae4c-9a1aff01a1a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <i class=\"fa fa-pencil\" aria-hidden=\"true\"></i>\n",
    "    <b>Checkpoint 2:</b>\n",
    "</div>\n",
    "\n",
    "Consider the two figures that you just generated. In the markdown cell below, take some notes on the similarities and differences between the timelines of the two runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32687e6e-9db8-4a02-955a-508401ae7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytercards import display_flashcards\n",
    "display_flashcards('../quiz-material/04-cp1-2.json')\n",
    "display_flashcards('../quiz-material/04-cp1-3.json')\n",
    "display_flashcards('../quiz-material/04-cp1-4.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4335a50-39fd-4579-aafc-26b6de21ede7",
   "metadata": {},
   "source": [
    "**Step 10:** Now let's try a GCB run with `--onlyAnn`. Before we do, we need to change our `workDir` and `outDir` paths in the `nextflow.config` so that it does not overwrite the output that we just created for the `--all` run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98521f-c608-49a6-9761-2c97d8203da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl -i.allgcb -pe 's#basicRun#onlyAnnRun#g' ./TransPi/nextflow.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39fb97-2a2c-4eeb-8cca-9ff6ed54afa6",
   "metadata": {},
   "source": [
    "**Step 11:** Time to run. The only change that we will make to the run command is to change `--all` to `--onlyAnn`\n",
    "> This run should take about **30 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d032206-02bc-473e-8b2e-c313a1f53e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!NXF_VER=22.10.1 ./nextflow run ./TransPi/TransPi.nf \\\n",
    "-profile gcb  --onlyAnn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e74889-f236-484a-975d-1c565bb08a49",
   "metadata": {},
   "source": [
    "Feel free to explore the results found in the GCB `onlyAnn` run. The following cell will place the `pipeline_info` directory from the run into the directory: `./GCBonlyAnnRun/output`. The rest of the results should be essentially the same as the `onlyAnn` run locally in Submodule 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489aca9-812c-4d6f-aa18-000bd72a5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir GCBonlyAnnRun\n",
    "!mkdir ./GCBonlyAnnRun/output\n",
    "!gsutil -m cp -r $gbPath/onlyAnnRun/output/pipeline_info ./GCBonlyAnnRun/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96722e89-2d6a-4381-ba42-673e9be79a2e",
   "metadata": {},
   "source": [
    "##### At this point, you have the toolkit necessary to run TransPi in various configurations and the baseline knowledge to interpret the output that TransPi produces. You also have the foundational knowledge of Google Cloud resources with the ability to utilize buckets and cloud computing to execute your computational task. Specifically, Batch which not only works with TransPi but also with any other Nextflow pipeline. We urge you to continue exploring TransPi, using different data sets, and also to explore other Nextflow pipelines as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213f6a1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This module demonstrated the execution of the TransPi transcriptome assembly workflow on Google Batch, a significant advancement from local Jupyter Notebook execution.  By leveraging Google Batch's scheduling capabilities, we achieved both cost efficiency through automated resource allocation and increased speed through parallelization of computational tasks.  The integration of Nextflow with Google Batch streamlined the process, requiring only minor adjustments to the `nextflow.config` file to redirect file paths to Google Cloud Storage (GCS) buckets and optimize compute allocations.  Comparison of local and Google Batch run timelines highlighted the benefits of cloud computing for large-scale bioinformatics analyses.  This learning module equipped users with the skills to effectively utilize Google Batch for efficient and scalable execution of Nextflow pipelines, paving the way for more complex and data-intensive bioinformatics projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661513f",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "You would proceed to the next notebook [`Submodule_05_Bonus_Notebook.ipynb`](./Submodule_05_Bonus_Notebook.ipynb) or shut down your instance if you are finished."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
